{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install git+https://github.com/openai/whisper.git\n",
    "# !pip3 install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2.88G/2.88G [00:23<00:00, 134MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import whisper\n",
    "import pandas as pd\n",
    "# import ffmpeg\n",
    "\n",
    "model = whisper.load_model(\"large\")\n",
    "\n",
    "def transcribe_Whisper_large(filename):\n",
    "    result = model.transcribe(filename, language='zh')\n",
    "    return result[\"text\"]\n",
    "\n",
    "df_whisper_large = pd.DataFrame({\n",
    "    'Filename': [],\n",
    "    'Start_time': [],\n",
    "    'End_time': [],\n",
    "    'Whisper_large': []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 71):\n",
    "    number = str(i).zfill(2)\n",
    "    input_files = [f for f in os.listdir(f\"segmented_audio/conversationA/00{number}\")]\n",
    "    for input_file in input_files:\n",
    "        predicted = transcribe_Whisper_large(f'segmented_audio/conversationA/00{number}/{input_file}')\n",
    "        match = re.search(r'(\\d+\\.\\d+)_(\\d+\\.\\d+)\\.wav$', input_file)\n",
    "        start_time = match.group(1)\n",
    "        end_time = match.group(2)\n",
    "        df_whisper_large.loc[len(df_whisper_large)] = {'Filename': f'D00{number}_A','Start_time': start_time,'End_time': end_time,'Whisper_large': predicted}\n",
    "\n",
    "for i in range(1, 71):\n",
    "    number = str(i).zfill(2)\n",
    "    input_files = [f for f in os.listdir(f\"segmented_audio/conversationB/00{number}\")]\n",
    "    for input_file in input_files:\n",
    "        predicted = transcribe_Whisper_large(f'segmented_audio/conversationB/00{number}/{input_file}')\n",
    "        match = re.search(r'(\\d+\\.\\d+)_(\\d+\\.\\d+)\\.wav$', input_file)\n",
    "        start_time = match.group(1)\n",
    "        end_time = match.group(2)\n",
    "        df_whisper_large.loc[len(df_whisper_large)] = {'Filename': f'D00{number}_B','Start_time': start_time,'End_time': end_time,'Whisper_large': predicted}\n",
    "\n",
    "for i in range(1, 71):\n",
    "    number = str(i).zfill(2)\n",
    "    input_files = [f for f in os.listdir(f\"segmented_audio/voice_command/00{number}\")]\n",
    "    for input_file in input_files:\n",
    "        predicted = transcribe_Whisper_large(f'segmented_audio/voice_command/00{number}/{input_file}')\n",
    "        match = re.search(r'(\\d+\\.\\d+)_(\\d+\\.\\d+)\\.wav$', input_file)\n",
    "        start_time = match.group(1)\n",
    "        end_time = match.group(2)\n",
    "        df_whisper_large.loc[len(df_whisper_large)] = {'Filename': f'P00{number}','Start_time': start_time,'End_time': end_time,'Whisper_large': predicted}\n",
    "\n",
    "df_whisper_large.to_csv('predicted_transcription/Whisper_large.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install jiwer\n",
    "!pip3 install cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_whisper_large = pd.read_csv('predicted_transcription/Whisper_large.csv', index_col=0)\n",
    "df_A_cleaned = pd.read_csv('ground_truth_cleaned/D00XX_A_ground_truth_cleaned.csv', index_col=0)\n",
    "df_B_cleaned = pd.read_csv('ground_truth_cleaned/D00XX_B_ground_truth_cleaned.csv', index_col=0)\n",
    "df_P_cleaned = pd.read_csv('ground_truth_cleaned/P00XX_ground_truth_cleaned.csv', index_col=0)\n",
    "df_whisper_large = df_whisper_large.rename(columns={'Filename': 'file', 'Start_time': 'start_time', 'End_time': 'end_time'})\n",
    "df_A_cleaned['file'] = df_A_cleaned['file'].str.extract(r'/(D\\d{4}_A)', expand=False)\n",
    "df_B_cleaned['file'] = df_B_cleaned['file'].str.extract(r'/(D\\d{4}_B)', expand=False)\n",
    "df_P_cleaned['file'] = df_P_cleaned['file'].str.extract(r'/(P\\d{4})', expand=False)\n",
    "df_merged = pd.merge(df_A_cleaned, df_whisper_large, on=['file', 'start_time', 'end_time'], how='outer')\n",
    "df_merged = pd.merge(df_B_cleaned, df_merged, on=['file', 'start_time', 'end_time'], how='right', suffixes=('_x',''))\n",
    "df_merged = df_merged.drop(columns=['participant_x', 'ground_truth_x', 'ground_truth_cleaned_x'])\n",
    "df_merged = pd.merge(df_merged, df_P_cleaned, on=['file', 'start_time', 'end_time'], how='outer', suffixes=('','_x'))\n",
    "df_merged['participant'] = df_merged['participant'].combine_first(df_merged['participant_x'])\n",
    "df_merged['ground_truth'] = df_merged['ground_truth'].combine_first(df_merged['ground_truth_x'])\n",
    "df_merged['ground_truth_cleaned'] = df_merged['ground_truth_cleaned'].combine_first(df_merged['ground_truth_cleaned_x'])\n",
    "df_merged = df_merged.drop(columns=['participant_x', 'ground_truth_x', 'ground_truth_cleaned_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /usr2/collab/dlee5/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import jiwer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sent2vec.vectorizer import Vectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "nltk.download('punkt')\n",
    "import jieba\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02de9438f688421ba56a3b5093af4482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3316f4b376174e02a22194299386f544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be58c38ab27456d8ea15800b734abff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073904efcfb9444b950912d302e7b377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg.8/academic-ml/spring-2024/install/spring-2024-pyt/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/share/pkg.8/academic-ml/spring-2024/install/spring-2024-pyt/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Bert distilbert-base-uncased\n",
      "Vectorization done on cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f40f88403ee4da988c286a2fb0acf31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edb1700d9b943a2abdd6f75043be93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18decfd767ff40e49db6a495554a9458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73db7221af72440e8e01c173ca8a8dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 87\u001b[0m\n\u001b[1;32m     85\u001b[0m         GloVe\u001b[39m.\u001b[39mappend(calculate_glove(reference, hypothesis))\n\u001b[1;32m     86\u001b[0m         BERT\u001b[39m.\u001b[39mappend(calculate_bert(reference, hypothesis))\n\u001b[0;32m---> 87\u001b[0m         Sent2Vec\u001b[39m.\u001b[39mappend(calculate_sent2vec(reference, hypothesis))\n\u001b[1;32m     89\u001b[0m df_merged[\u001b[39m'\u001b[39m\u001b[39mWER\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m WER\n\u001b[1;32m     90\u001b[0m df_merged[\u001b[39m'\u001b[39m\u001b[39mCER\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m CER\n",
      "Cell \u001b[0;32mIn[5], line 57\u001b[0m, in \u001b[0;36mcalculate_sent2vec\u001b[0;34m(reference, hypothesis)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_sent2vec\u001b[39m(reference, hypothesis):\n\u001b[1;32m     56\u001b[0m     vectorizer \u001b[39m=\u001b[39m Vectorizer()\n\u001b[0;32m---> 57\u001b[0m     vectorizer\u001b[39m.\u001b[39;49mrun([reference, hypothesis,])\n\u001b[1;32m     58\u001b[0m     vectors \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mvectors\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m cosine(vectors[\u001b[39m0\u001b[39m], vectors[\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sent2vec/vectorizer.py:43\u001b[0m, in \u001b[0;36mVectorizer.run\u001b[0;34m(self, sentences, remove_stop_words, add_stop_words)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAll items must be string type but \u001b[39m\u001b[39m{\u001b[39;00msentence\u001b[39m}\u001b[39;00m\u001b[39m is type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(sentence)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[39m# RUN\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectorizer\u001b[39m.\u001b[39;49m_execute(sentences, remove_stop_words\u001b[39m=\u001b[39;49mremove_stop_words, add_stop_words\u001b[39m=\u001b[39;49madd_stop_words)\n\u001b[1;32m     44\u001b[0m vectors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectorizer\u001b[39m.\u001b[39mvectors\n\u001b[1;32m     45\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(vectors\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sent2vec/vectorizer.py:77\u001b[0m, in \u001b[0;36mBertVectorizer._execute\u001b[0;34m(self, sentences, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_execute\u001b[39m(\u001b[39mself\u001b[39m, sentences, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 77\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     78\u001b[0m     model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     79\u001b[0m     tokenized \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mencode(x, add_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), sentences))\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/spring-2024/install/spring-2024-pyt/lib/python3.11/site-packages/transformers/modeling_utils.py:2058\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2053\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2054\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2055\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2056\u001b[0m     )\n\u001b[1;32m   2057\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2058\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/spring-2024/install/spring-2024-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/spring-2024/install/spring-2024-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/spring-2024/install/spring-2024-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/spring-2024/install/spring-2024-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/spring-2024/install/spring-2024-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# https://www.nltk.org/howto/wordnet.html\n",
    "# https://stackoverflow.com/questions/63514884/does-wordnet-python-nltk-interface-includes-any-measure-of-semantic-relatedness\n",
    "def calculate_wordnet(reference, hypothesis):\n",
    "    # ref_lemmas = [lemmas for word in jieba.lcut(reference) for lemmas in wn.lemmas(word, lang='cmn')] # consider all candidates\n",
    "    # hyp_lemmas = [lemmas for word in jieba.lcut(hypothesis) for lemmas in wn.lemmas(word, lang='cmn')]\n",
    "    ref_lemmas = [wn.lemmas(word, lang='cmn')[0] for word in jieba.lcut(reference) if wn.lemmas(word, lang='cmn')] # only top 1\n",
    "    hyp_lemmas = [wn.lemmas(word, lang='cmn')[0] for word in jieba.lcut(hypothesis) if wn.lemmas(word, lang='cmn')]\n",
    "    similarities = []\n",
    "    for ref_lemma in ref_lemmas:\n",
    "        for hyp_lemma in hyp_lemmas:\n",
    "            similarity = wn.wup_similarity(ref_lemma.synset(), hyp_lemma.synset())\n",
    "            if similarity:\n",
    "                similarities.append(similarity)\n",
    "    if similarities:\n",
    "        return sum(similarities) / len(similarities)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# https://github.com/stanfordnlp/GloVe # only has English\n",
    "## https://github.com/Embedding/Chinese-Word-Vectors # convert embeddings to this one\n",
    "glove_embeddings = {}\n",
    "with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n",
    "def calculate_glove(reference, hypothesis):\n",
    "    ref_vectors = [glove_embeddings[word] for word in jieba.lcut(reference) if word in glove_embeddings]\n",
    "    hyp_vectors = [glove_embeddings[word] for word in jieba.lcut(hypothesis) if word in glove_embeddings]\n",
    "    if not ref_vectors or not hyp_vectors:\n",
    "        return 0.0\n",
    "    ref_mean_vector = np.mean(ref_vectors, axis=0).reshape(1, -1)\n",
    "    hyp_mean_vector = np.mean(hyp_vectors, axis=0).reshape(1, -1)\n",
    "    return cosine_similarity(ref_mean_vector, hyp_mean_vector)[0][0]\n",
    "\n",
    "\n",
    "# https://huggingface.co/google-bert/bert-base-chinese\n",
    "## https://github.com/ymcui/Chinese-BERT-wwm/blob/master/README_EN.md # advanced CWS included version\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "bert_model = BertModel.from_pretrained('bert-base-chinese')\n",
    "def calculate_bert(reference, hypothesis):\n",
    "    ref_inputs = bert_tokenizer(reference, return_tensors='pt')\n",
    "    hyp_inputs = bert_tokenizer(hypothesis, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        ref_outputs = bert_model(**ref_inputs)\n",
    "        hyp_outputs = bert_model(**hyp_inputs)\n",
    "    ref_embedding = ref_outputs.last_hidden_state.mean(dim=1)\n",
    "    hyp_embedding = hyp_outputs.last_hidden_state.mean(dim=1)\n",
    "    return cosine_similarity(ref_embedding, hyp_embedding)[0][0].item()\n",
    "\n",
    "\n",
    "# https://pypi.org/project/sent2vec/\n",
    "def calculate_sent2vec(reference, hypothesis):\n",
    "    vectorizer = Vectorizer()\n",
    "    vectorizer.run([reference, hypothesis,])\n",
    "    vectors = vectorizer.vectors\n",
    "    return cosine(vectors[0], vectors[1])\n",
    "\n",
    "\n",
    "WER = []\n",
    "CER = []\n",
    "BLEU = []\n",
    "WordNet = []\n",
    "GloVe = []\n",
    "BERT = []\n",
    "Sent2Vec = []\n",
    "for i in range(df_merged.shape[0]):\n",
    "    reference = df_merged['ground_truth_cleaned'].iloc[i]\n",
    "    hypothesis = df_merged['Whisper_large'].iloc[i]\n",
    "    if not isinstance(reference, str) or not isinstance(hypothesis, str):\n",
    "        WER.append(np.nan)\n",
    "        CER.append(np.nan)\n",
    "        BLEU.append(np.nan)\n",
    "        WordNet.append(np.nan)\n",
    "        GloVe.append(np.nan)\n",
    "        BERT.append(np.nan)\n",
    "        Sent2Vec.append(np.nan)\n",
    "    else:\n",
    "        WER.append(jiwer.wer(' '.join(jieba.lcut(reference)), ' '.join(jieba.lcut(hypothesis))))\n",
    "        CER.append(jiwer.cer(reference, hypothesis))\n",
    "        BLEU.append(sentence_bleu([jieba.lcut(reference)], jieba.lcut(hypothesis)))\n",
    "        WordNet.append(calculate_wordnet(reference, hypothesis))\n",
    "        GloVe.append(calculate_glove(reference, hypothesis))\n",
    "        BERT.append(calculate_bert(reference, hypothesis))\n",
    "        Sent2Vec.append(calculate_sent2vec(reference, hypothesis))\n",
    "    \n",
    "df_merged['WER'] = WER\n",
    "df_merged['CER'] = CER\n",
    "df_merged['BLEU'] = BLEU\n",
    "df_merged['WordNet'] = WordNet\n",
    "df_merged['GloVe'] = GloVe\n",
    "df_merged['BERT'] = BERT\n",
    "df_merged['Sent2Vec'] = Sent2Vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
