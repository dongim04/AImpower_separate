{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install git+https://github.com/openai/whisper.git\n",
    "# !pip3 install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import whisper\n",
    "import pandas as pd\n",
    "# import ffmpeg\n",
    "\n",
    "model = whisper.load_model(\"tiny\")\n",
    "\n",
    "def transcribe_Whisper_tiny(filename):\n",
    "    result = model.transcribe(filename, language='zh')\n",
    "    return result[\"text\"]\n",
    "\n",
    "df_whisper_tiny = pd.DataFrame({\n",
    "    'Filename': [],\n",
    "    'Start_time': [],\n",
    "    'End_time': [],\n",
    "    'Whisper_tiny': []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 71):\n",
    "    number = str(i).zfill(2)\n",
    "    input_files = [f for f in os.listdir(f\"segmented_audio/conversationA/00{number}\")]\n",
    "    for input_file in input_files:\n",
    "        predicted = transcribe_Whisper_tiny(f'segmented_audio/conversationA/00{number}/{input_file}')\n",
    "        match = re.search(r'(\\d+\\.\\d+)_(\\d+\\.\\d+)\\.wav$', input_file)\n",
    "        start_time = match.group(1)\n",
    "        end_time = match.group(2)\n",
    "        df_whisper_tiny.loc[len(df_whisper_tiny)] = {'Filename': f'D00{number}_A','Start_time': start_time,'End_time': end_time,'Whisper_tiny': predicted}\n",
    "\n",
    "for i in range(1, 71):\n",
    "    number = str(i).zfill(2)\n",
    "    input_files = [f for f in os.listdir(f\"segmented_audio/conversationB/00{number}\")]\n",
    "    for input_file in input_files:\n",
    "        predicted = transcribe_Whisper_tiny(f'segmented_audio/conversationB/00{number}/{input_file}')\n",
    "        match = re.search(r'(\\d+\\.\\d+)_(\\d+\\.\\d+)\\.wav$', input_file)\n",
    "        start_time = match.group(1)\n",
    "        end_time = match.group(2)\n",
    "        df_whisper_tiny.loc[len(df_whisper_tiny)] = {'Filename': f'D00{number}_B','Start_time': start_time,'End_time': end_time,'Whisper_tiny': predicted}\n",
    "\n",
    "for i in range(1, 71):\n",
    "    number = str(i).zfill(2)\n",
    "    input_files = [f for f in os.listdir(f\"segmented_audio/voice_command/00{number}\")]\n",
    "    for input_file in input_files:\n",
    "        predicted = transcribe_Whisper_tiny(f'segmented_audio/voice_command/00{number}/{input_file}')\n",
    "        match = re.search(r'(\\d+\\.\\d+)_(\\d+\\.\\d+)\\.wav$', input_file)\n",
    "        start_time = match.group(1)\n",
    "        end_time = match.group(2)\n",
    "        df_whisper_tiny.loc[len(df_whisper_tiny)] = {'Filename': f'P00{number}','Start_time': start_time,'End_time': end_time,'Whisper_tiny': predicted}\n",
    "\n",
    "df_whisper_tiny.to_csv('predicted_transcription/Whisper-tiny.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in /restricted/projectnb/sparkpit/dlee5/.conda/envs/my_conda_env2/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /restricted/projectnb/sparkpit/dlee5/.conda/envs/my_conda_env2/lib/python3.9/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /restricted/projectnb/sparkpit/dlee5/.conda/envs/my_conda_env2/lib/python3.9/site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /restricted/projectnb/sparkpit/dlee5/.conda/envs/my_conda_env2/lib/python3.9/site-packages (from nltk) (4.66.4)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install jiwer\n",
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_whisper_tiny = pd.read_csv('predicted_transcription/Whisper-tiny.csv', index_col=0)\n",
    "df_A_cleaned = pd.read_csv('ground_truth_cleaned/D00XX_A_ground_truth_cleaned.csv', index_col=0)\n",
    "df_B_cleaned = pd.read_csv('ground_truth_cleaned/D00XX_B_ground_truth_cleaned.csv', index_col=0)\n",
    "df_P_cleaned = pd.read_csv('ground_truth_cleaned/P00XX_ground_truth_cleaned.csv', index_col=0)\n",
    "df_whisper_tiny = df_whisper_tiny.rename(columns={'Filename': 'file', 'Start_time': 'start_time', 'End_time': 'end_time'})\n",
    "df_A_cleaned['file'] = df_A_cleaned['file'].str.extract(r'/(D\\d{4}_A)', expand=False)\n",
    "df_B_cleaned['file'] = df_B_cleaned['file'].str.extract(r'/(D\\d{4}_B)', expand=False)\n",
    "df_P_cleaned['file'] = df_P_cleaned['file'].str.extract(r'/(P\\d{4})', expand=False)\n",
    "df_merged = pd.merge(df_A_cleaned, df_whisper_tiny, on=['file', 'start_time', 'end_time'], how='outer')\n",
    "df_merged = pd.merge(df_B_cleaned, df_merged, on=['file', 'start_time', 'end_time'], how='right', suffixes=('_x',''))\n",
    "df_merged = df_merged.drop(columns=['participant_x', 'ground_truth_x', 'ground_truth_cleaned_x'])\n",
    "df_merged = pd.merge(df_merged, df_P_cleaned, on=['file', 'start_time', 'end_time'], how='outer', suffixes=('','_x'))\n",
    "df_merged['participant'] = df_merged['participant'].combine_first(df_merged['participant_x'])\n",
    "df_merged['ground_truth'] = df_merged['ground_truth'].combine_first(df_merged['ground_truth_x'])\n",
    "df_merged['ground_truth_cleaned'] = df_merged['ground_truth_cleaned'].combine_first(df_merged['ground_truth_cleaned_x'])\n",
    "df_merged = df_merged.drop(columns=['participant_x', 'ground_truth_x', 'ground_truth_cleaned_x'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rprojectnb/sparkpit/dlee5/.conda/envs/my_conda_env2/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/rprojectnb/sparkpit/dlee5/.conda/envs/my_conda_env2/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/rprojectnb/sparkpit/dlee5/.conda/envs/my_conda_env2/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "#wordnet similarity, glove, bert, sent2vec\n",
    "import jiwer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "WER = []\n",
    "CER = []\n",
    "BLEU = []\n",
    "for i in range(df_merged.shape[0]):\n",
    "    reference = df_merged['ground_truth_cleaned'].iloc[i]\n",
    "    hypothesis = df_merged['Whisper_tiny'].iloc[i]\n",
    "    if not isinstance(reference, str) or not isinstance(hypothesis, str):\n",
    "        WER.append(np.nan)\n",
    "        CER.append(np.nan)\n",
    "        BLEU.append(np.nan)\n",
    "    else:\n",
    "        WER.append(jiwer.wer(reference, hypothesis))\n",
    "        CER.append(jiwer.cer(reference, hypothesis))\n",
    "        BLEU.append(sentence_bleu([list(reference)], list(hypothesis)))\n",
    "df_merged['WER'] = WER\n",
    "df_merged['CER'] = CER\n",
    "df_merged['BLEU'] = BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>participant</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>ground_truth_cleaned</th>\n",
       "      <th>Whisper_tiny</th>\n",
       "      <th>WER</th>\n",
       "      <th>CER</th>\n",
       "      <th>BLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D0001_A</td>\n",
       "      <td>87.970</td>\n",
       "      <td>94.850</td>\n",
       "      <td>1.0</td>\n",
       "      <td>大/r[大]家好，我是叫&lt;姓名&gt;，那。</td>\n",
       "      <td>大家好，我是叫&lt;姓名&gt;，那。</td>\n",
       "      <td>icas良好的大家好</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.473563e-78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D0001_A</td>\n",
       "      <td>95.580</td>\n",
       "      <td>102.830</td>\n",
       "      <td>1.0</td>\n",
       "      <td>我是，今年是，&lt;年龄&gt;岁来自&lt;居住地&gt;，呃/i。</td>\n",
       "      <td>我是，今年是，&lt;年龄&gt;岁来自&lt;居住地&gt;，。</td>\n",
       "      <td>我是金莲是</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>3.784014e-156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D0001_A</td>\n",
       "      <td>104.090</td>\n",
       "      <td>105.680</td>\n",
       "      <td>1.0</td>\n",
       "      <td>我是。</td>\n",
       "      <td>我是。</td>\n",
       "      <td>爸...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D0001_A</td>\n",
       "      <td>106.300</td>\n",
       "      <td>119.120</td>\n",
       "      <td>1.0</td>\n",
       "      <td>资深/p的口/b[口/r/b]吃患者。</td>\n",
       "      <td>资深的口吃患者。</td>\n",
       "      <td>为什么蛮 perceive Freaky</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D0001_A</td>\n",
       "      <td>119.130</td>\n",
       "      <td>124.490</td>\n",
       "      <td>1.0</td>\n",
       "      <td>我是从小就有口吃。</td>\n",
       "      <td>我是从小就有口吃。</td>\n",
       "      <td>我吃 藏下就有口吃</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>3.155985e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37252</th>\n",
       "      <td>P0070</td>\n",
       "      <td>3032.636</td>\n",
       "      <td>3035.926</td>\n",
       "      <td>70.0</td>\n",
       "      <td>你好，米雅，拿日元换法郎怎么换。</td>\n",
       "      <td>你好，米雅，拿日元换法郎怎么换。</td>\n",
       "      <td>你好好米亞,拿日元換法了,這塊</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>3.383742e-78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37253</th>\n",
       "      <td>P0070</td>\n",
       "      <td>3037.766</td>\n",
       "      <td>3041.436</td>\n",
       "      <td>70.0</td>\n",
       "      <td>你好，米雅，六百五十韩元是多少法郎。</td>\n",
       "      <td>你好，米雅，六百五十韩元是多少法郎。</td>\n",
       "      <td>你好,明呀,653元是多少法狼</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>2.316647e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37254</th>\n",
       "      <td>P0070</td>\n",
       "      <td>3043.086</td>\n",
       "      <td>3045.546</td>\n",
       "      <td>70.0</td>\n",
       "      <td>你好，米雅，添加日程。</td>\n",
       "      <td>你好，米雅，添加日程。</td>\n",
       "      <td>你好,我们呀,天家日程</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>7.746340e-155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37255</th>\n",
       "      <td>P0070</td>\n",
       "      <td>3049.256</td>\n",
       "      <td>3053.116</td>\n",
       "      <td>70.0</td>\n",
       "      <td>我觉得推翻现有数学体系不是那么不可思议了。</td>\n",
       "      <td>我觉得推翻现有数学体系不是那么不可思议了。</td>\n",
       "      <td>我覺得推翻現有數學體系不是那麼不可思議了</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>2.250010e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37256</th>\n",
       "      <td>P0070</td>\n",
       "      <td>3054.786</td>\n",
       "      <td>3057.266</td>\n",
       "      <td>70.0</td>\n",
       "      <td>学生的质量出现明显差异。</td>\n",
       "      <td>学生的质量出现明显差异。</td>\n",
       "      <td>學生的資料出現明顯差異</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>6.289053e-155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37257 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file  start_time  end_time  participant              ground_truth  \\\n",
       "0      D0001_A      87.970    94.850          1.0       大/r[大]家好，我是叫<姓名>，那。   \n",
       "1      D0001_A      95.580   102.830          1.0  我是，今年是，<年龄>岁来自<居住地>，呃/i。   \n",
       "2      D0001_A     104.090   105.680          1.0                       我是。   \n",
       "3      D0001_A     106.300   119.120          1.0       资深/p的口/b[口/r/b]吃患者。   \n",
       "4      D0001_A     119.130   124.490          1.0                 我是从小就有口吃。   \n",
       "...        ...         ...       ...          ...                       ...   \n",
       "37252    P0070    3032.636  3035.926         70.0          你好，米雅，拿日元换法郎怎么换。   \n",
       "37253    P0070    3037.766  3041.436         70.0        你好，米雅，六百五十韩元是多少法郎。   \n",
       "37254    P0070    3043.086  3045.546         70.0               你好，米雅，添加日程。   \n",
       "37255    P0070    3049.256  3053.116         70.0     我觉得推翻现有数学体系不是那么不可思议了。   \n",
       "37256    P0070    3054.786  3057.266         70.0              学生的质量出现明显差异。   \n",
       "\n",
       "        ground_truth_cleaned          Whisper_tiny  WER       CER  \\\n",
       "0             大家好，我是叫<姓名>，那。            icas良好的大家好  1.0  1.000000   \n",
       "1      我是，今年是，<年龄>岁来自<居住地>，。                 我是金莲是  1.0  0.857143   \n",
       "2                        我是。                  爸...  1.0  1.333333   \n",
       "3                   资深的口吃患者。  为什么蛮 perceive Freaky  3.0  2.500000   \n",
       "4                  我是从小就有口吃。             我吃 藏下就有口吃  2.0  0.555556   \n",
       "...                      ...                   ...  ...       ...   \n",
       "37252       你好，米雅，拿日元换法郎怎么换。       你好好米亞,拿日元換法了,這塊  1.0  0.562500   \n",
       "37253     你好，米雅，六百五十韩元是多少法郎。       你好,明呀,653元是多少法狼  1.0  0.611111   \n",
       "37254            你好，米雅，添加日程。           你好,我们呀,天家日程  1.0  0.727273   \n",
       "37255  我觉得推翻现有数学体系不是那么不可思议了。  我覺得推翻現有數學體系不是那麼不可思議了  1.0  0.380952   \n",
       "37256           学生的质量出现明显差异。           學生的資料出現明顯差異  1.0  0.583333   \n",
       "\n",
       "                BLEU  \n",
       "0       2.473563e-78  \n",
       "1      3.784014e-156  \n",
       "2       0.000000e+00  \n",
       "3       0.000000e+00  \n",
       "4       3.155985e-01  \n",
       "...              ...  \n",
       "37252   3.383742e-78  \n",
       "37253   2.316647e-01  \n",
       "37254  7.746340e-155  \n",
       "37255   2.250010e-01  \n",
       "37256  6.289053e-155  \n",
       "\n",
       "[37257 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
